#!/bin/bash

# ===========================================
# GGUF Quantization, Metadata & Auto-Upload
# With Per-Model Cards + MODELFILE + HF Upload
# Geoff Munn / 2025
# ===========================================

set -e  # Exit on any error

# -------------------------------
# CONFIGURE THESE VALUES
# -------------------------------

INPUT_PRECISION="f16"
QUANTS=("Q2_K" "Q3_K_S" "Q3_K_M" "Q4_K_S" "Q4_K_M" "Q5_K_S" "Q5_K_M" "Q6_K" "Q8_0")

MODEL_NAME="Qwen3-0.6B"
HF_REPO="geoffmunn/${MODEL_NAME}"
HF_REPO_URL="https://huggingface.co/${HF_REPO}"

BASE_REPO="Qwen/Qwen3-0.6B"
LICENSE="apache-2.0"
OUTPUT_DIR="./dist"

QUANTIZE_BIN="./build/bin/llama-quantize"
COMMIT_MSG="Add Q2â€“Q8_0 quantized models with per-model cards, MODELFILE, CLI examples, and auto-upload"

# -------------------------------
# DERIVE INPUT & OUTPUT NAMES
# -------------------------------

INPUT_MODEL="${PWD}/${MODEL_NAME}-${INPUT_PRECISION}.gguf"

if [ ! -f "$INPUT_MODEL" ]; then
    echo "âŒ Error: Input model '$INPUT_MODEL' not found!"
    if [ -f "../$INPUT_MODEL" ]; then
        echo "âœ… Found ../$INPUT_MODEL â€” using that."
        INPUT_MODEL="../$INPUT_MODEL"
    else
        echo "âŒ Also checked ../$INPUT_MODEL â€” not found."
        exit 1
    fi
fi

mkdir -p "$OUTPUT_DIR"
cd "$OUTPUT_DIR"

echo "âœ… Starting GGUF preparation..."
echo "   Input: $INPUT_MODEL"
echo "   Output dir: $(pwd)"
echo "   Source precision: $INPUT_PRECISION"
echo "   Target quants: ${QUANTS[*]}"

# -------------------------------
# STEP 1: Quantize Models
# -------------------------------

for QTYPE in "${QUANTS[@]}"; do
    OUTPUT_FILE="${MODEL_NAME}-${INPUT_PRECISION}:${QTYPE}.gguf"
    
    if [ -f "$OUTPUT_FILE" ]; then
        echo "ğŸ’¡ $OUTPUT_FILE already exists, skipping..."
        continue
    fi

    echo "ğŸ“¦ Quantizing ${INPUT_PRECISION} â†’ $QTYPE â†’ $OUTPUT_FILE"
    "../$QUANTIZE_BIN" "$INPUT_MODEL" "$OUTPUT_FILE" "$QTYPE"

    if [ $? -ne 0 ]; then
        echo "   âŒ Failed to quantize to $QTYPE"
        exit 1
    fi

    # Validate output is a real GGUF file
    if [ ! -s "$OUTPUT_FILE" ] || ! head -c 4 "$OUTPUT_FILE" | grep -q "GGUF"; then
        echo "ğŸ’¥ ERROR: $OUTPUT_FILE is not a valid GGUF file (invalid magic or empty)"
        exit 1
    fi
    echo "   âœ… Success: $OUTPUT_FILE created and validated"
done

echo

# -------------------------------
# STEP 2: Generate SHA256SUMS.txt
# -------------------------------

echo "ğŸ” Generating SHA256SUMS.txt..."
sha256sum *.gguf > SHA256SUMS.txt
echo "âœ… SHA256 checksums:"
cat SHA256SUMS.txt
echo

# -------------------------------
# STEP 3: Generate Main README.md (Hub Index)
# -------------------------------

cat > README.md << 'EOF'
---
license: apache-2.0
tags:
  - gguf
  - qwen
  - llama.cpp
  - quantized
  - text-generation
  - chat
  - edge-ai
  - tiny-model
base_model: BASE_REPO
author: geoffmunn
pipeline_tag: text-generation
language:
  - en
  - zh
---

# MODEL_NAME-GGUF

This is a **GGUF-quantized version** of the **[BASE_REPO](https://huggingface.co/BASE_REPO)** language model â€” a compact **600-million-parameter** LLM designed for **ultra-fast inference on low-resource devices**.

Converted for use with `llama.cpp`, [LM Studio](https://lmstudio.ai), [OpenWebUI](https://openwebui.com), and [GPT4All](https://gpt4all.io), enabling private AI anywhere â€” even offline.

> âš ï¸ **Note**: This is a *very small* model. It will not match larger models (e.g., 4B+) in reasoning, coding, or factual accuracy. However, it shines in **speed, portability, and efficiency**.

## Available Quantizations (from INPUT_PRECISION)

These variants were built from a **INPUT_PRECISION** base model to ensure consistency across quant levels.

| Level     | Quality       | Speed     | Size      | Recommendation |
|----------|--------------|----------|-----------|----------------|
| Q2_K     | Minimal      | âš¡ Fastest | 347 MB   | Use only on severely constrained systems (e.g., Raspberry Pi). Severely degraded output. |
| Q3_K_S   | Low          | âš¡ Fast    | 390 MB   | Barely usable; slight improvement over Q2_K. Avoid unless space-limited. |
| Q3_K_M   | Low-Medium   | âš¡ Fast    | 414 MB   | Usable for simple prompts on older CPUs. Acceptable for basic chat. |
| Q4_K_S   | Medium       | ğŸš€ Fast    | 471 MB   | Good balance for low-end devices. Recommended for embedded or mobile use. |
| Q4_K_M   | âœ… Practical  | ğŸš€ Fast    | 484 MB   | Best overall choice for most users. Solid performance on weak hardware. |
| Q5_K_S   | High         | ğŸ¢ Medium  | 544 MB   | Slight quality gain; good for testing or when extra fidelity matters. |
| Q5_K_M   | ğŸ”º Max Reasoning | ğŸ¢ Medium | 551 MB | Best quality available for this model. Use if you need slightly better logic or coherence. |
| Q6_K     | Near-FP16    | ğŸŒ Slow    | 623 MB   | Diminishing returns. Only use if full consistency is critical and RAM allows. |
| Q8_0     | Lossless*    | ğŸŒ Slow    | 805 MB   | Maximum fidelity, but gains are minor due to model size. Ideal for archival or benchmarking. |

> ğŸ’¡ **Recommendations by Use Case**
>
> - ğŸ“± **Mobile/Embedded/IoT Devices**: `Q4_K_S` or `Q4_K_M`
> - ğŸ’» **Old Laptops or Low-RAM Systems (<4GB RAM)**: `Q4_K_M`
> - ğŸ–¥ï¸ **Standard PCs/Macs (General Use)**: `Q5_K_M` (best quality)
> - âš™ï¸ **Ultra-Fast Inference Needs**: `Q3_K_M` or `Q4_K_S` (lowest latency)
> - ğŸ§© **Prompt Prototyping or UI Testing**: Any variant â€“ great for fast iteration
> - ğŸ› ï¸ **Development & Benchmarking**: Test from `Q4_K_M` up to `Q8_0` to assess trade-offs
> - âŒ **Avoid For**: Complex reasoning, math, code generation, fact-heavy tasks

## Why Use a 0.6B Model?

While limited in capability compared to larger models, **Qwen3-0.6B** excels at:
- Running **instantly** on CPUs without GPU
- Fitting into **<2GB RAM**, even when quantized
- Enabling **offline AI on microcontrollers, phones, or edge devices**
- Serving as a **fast baseline** for lightweight NLP tasks (intent detection, short responses)

Itâ€™s ideal for:
- Chatbots with simple flows
- On-device assistants
- Educational demos
- Rapid prototyping

## Usage

Load this model using:
- [OpenWebUI](https://openwebui.com) â€“ self-hosted, extensible interface
- [LM Studio](https://lmstudio.ai) â€“ local LLM desktop app
- [GPT4All](https://gpt4all.io) â€“ private, local AI chatbot
- Or directly via \`llama.cpp\`

Each model includes its own `README.md` and `MODELFILE` for optimal configuration.

## Author

ğŸ‘¤ Geoff Munn (@geoffmunn)  
ğŸ”— [Hugging Face Profile](https://huggingface.co/geoffmunn)

## Disclaimer

This is a community conversion for local inference. Not affiliated with Alibaba Cloud or the Qwen team.
EOF

sed -i "s|MODEL_NAME|$MODEL_NAME|g" README.md
sed -i "s|BASE_REPO|$BASE_REPO|g" README.md
sed -i "s|LICENSE|$LICENSE|g" README.md
sed -i "s|INPUT_PRECISION|$INPUT_PRECISION|g" README.md

echo "âœ… Main README.md (hub index) generated!"

# -------------------------------
# STEP 4: Generate Per-Model README Cards
# -------------------------------

declare -A RECOMMENDATIONS
RECOMMENDATIONS["Q2_K"]="Minimal quality; only for extreme memory constraints. Output may be incoherent."
RECOMMENDATIONS["Q3_K_S"]="Low quality; barely usable. Suitable only for keyword extraction or token streaming tests."
RECOMMENDATIONS["Q3_K_M"]="Acceptable for basic interaction on legacy hardware. Simple chat OK."
RECOMMENDATIONS["Q4_K_S"]="Solid mid-low tier. Great for quick replies on mobile or embedded platforms."
RECOMMENDATIONS["Q4_K_M"]="Best speed/quality trade-off. Recommended for general-purpose usage."
RECOMMENDATIONS["Q5_K_S"]="High-quality for a 0.6B model. Slightly slower, better coherence."
RECOMMENDATIONS["Q5_K_M"]="Highest practical quality. Choose this if you want the best possible output."
RECOMMENDATIONS["Q6_K"]="Near-lossless. Minor gains over Q5_K_M. Use only if memory isn't tight."
RECOMMENDATIONS["Q8_0"]="Full precision (lossless). Ideal for reproducibility, research, or archiving."

for QTYPE in "${QUANTS[@]}"; do
    MODEL_FILE="${MODEL_NAME}-${INPUT_PRECISION}:${QTYPE}.gguf"
    
    if [ ! -f "$MODEL_FILE" ]; then
        echo "âš ï¸ Skipping card for $MODEL_FILE â€” not found"
        continue
    fi

    DIRNAME="${MODEL_NAME}-${QTYPE}"
    mkdir -p "$DIRNAME"

    FILE_SIZE=$(ls -lh "$MODEL_FILE" | awk '{print $5}')

    # Estimate RAM and performance
    case "$QTYPE" in
        "Q2_K")   ram="~0.6 GB"; speed="âš¡ Fast"; qual="Minimal" ;;
        "Q3_K_S") ram="~0.7 GB"; speed="âš¡ Fast"; qual="Low" ;;
        "Q3_K_M") ram="~0.8 GB"; speed="âš¡ Fast"; qual="Low-Medium" ;;
        "Q4_K_S") ram="~0.9 GB"; speed="ğŸš€ Fast"; qual="Medium" ;;
        "Q4_K_M") ram="~1.0 GB"; speed="ğŸš€ Fast"; qual="Practical" ;;
        "Q5_K_S") ram="~1.1 GB"; speed="ğŸ¢ Medium"; qual="High" ;;
        "Q5_K_M") ram="~1.2 GB"; speed="ğŸ¢ Medium"; qual="Max Reasoning" ;;
        "Q6_K")   ram="~1.4 GB"; speed="ğŸŒ Slow"; qual="Near-FP16" ;;
        "Q8_0")   ram="~1.7 GB"; speed="ğŸŒ Slow"; qual="Lossless*" ;;
        *)        ram="~? GB"; speed="â“ Unknown"; qual="Unknown" ;;
    esac

    # Dynamic CLI example based on model capability
    if [[ "$QTYPE" =~ ^(Q5_K_M|Q6_K|Q8_0)$ ]]; then
        PROMPT="Explain what gravity is in one sentence suitable for a child."
        TEMP=0.6
        MODE="general"
    elif [[ "$QTYPE" =~ ^(Q4_K_M|Q5_K_S)$ ]]; then
        PROMPT="Write a short joke about cats."
        TEMP=0.8  # Higher temp for creativity
        MODE="creative"
    else
        PROMPT="Repeat the word 'hello' five times separated by commas."
        TEMP=0.1  # Deterministic for low-quants
        MODE="basic"
    fi

    cat > "$DIRNAME/README.md" << EOF
---
license: apache-2.0
tags:
  - gguf
  - qwen
  - llama.cpp
  - quantized
  - text-generation
  - chat
  - edge-ai
  - tiny-model
base_model: $BASE_REPO
author: geoffmunn
---

# ${MODEL_NAME}-${QTYPE}

Quantized version of [${BASE_REPO}](https://huggingface.co/${BASE_REPO}) at **${QTYPE}** level, derived from **${INPUT_PRECISION}** base weights.

## Model Info

- **Format**: GGUF (for llama.cpp and compatible runtimes)
- **Size**: ${FILE_SIZE}
- **Precision**: ${QTYPE}
- **Base Model**: [${BASE_REPO}](https://huggingface.co/${BASE_REPO})
- **Conversion Tool**: [llama.cpp](https://github.com/ggerganov/llama.cpp)

## Quality & Performance

| Metric | Value |
|-------|-------|
| **Quality** | ${qual} |
| **Speed** | ${speed} |
| **RAM Required** | ${ram} |
| **Recommendation** | ${RECOMMENDATIONS[$QTYPE]} |

## Prompt Template (ChatML)

This model uses the **ChatML** format used by Qwen:

\`\`\`text
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
\`\`\`

Set this in your app (LM Studio, OpenWebUI, etc.) for best results.

## Generation Parameters

Recommended defaults:

| Parameter | Value |
|---------|-------|
| Temperature | 0.6 |
| Top-P | 0.95 |
| Top-K | 20 |
| Min-P | 0.0 |
| Repeat Penalty | 1.1 |

Stop sequences: \`<|im_end|>\`, \`<|im_start|>\`

> âš ï¸ Due to model size, avoid temperatures above 0.9 â€” outputs become highly unpredictable.

## ğŸ’¡ Usage Tips

> This model is best suited for lightweight tasks:
>
> ### âœ… Ideal Uses
> - Quick replies and canned responses
> - Intent classification (e.g., â€œIs this user asking for help?â€)
> - UI prototyping and local AI testing
> - Embedded/NPU deployment
>
> ### âŒ Limitations
> - No complex reasoning or multi-step logic
> - Poor math and code generation
> - Limited world knowledge
> - May repeat or hallucinate frequently at higher temps
>
> ---
>
> ğŸ”„ **Fast Iteration Friendly**  
> Perfect for developers building prompt templates or testing UI integrations.
>
> ğŸ”‹ **Runs on Almost Anything**  
> Even Raspberry Pi Zero W can run Q2_K with swap enabled.
>
> ğŸ“¦ **Tiny Footprint**  
> Fits easily on USB drives, microSD cards, or IoT devices.

## ğŸ–¥ï¸ CLI Example Using Ollama or TGI Server

Hereâ€™s how you can query this model via API using \`curl\` and \`jq\`. Replace the endpoint with your local server (e.g., Ollama, Text Generation Inference).

\`\`\`bash
curl http://localhost:11434/api/generate -s -N -d '{
  "model": "hf.co/geoffmunn/Qwen3-0.6B:${QTYPE}",
  "prompt": "Respond exactly as follows: ${PROMPT}",
  "temperature": ${TEMP},
  "top_p": 0.95,
  "top_k": 20,
  "min_p": 0.0,
  "repeat_penalty": 1.1,
  "stream": false
}' | jq -r '.response'
\`\`\`

ğŸ¯ **Why this works well**:
- The prompt is meaningful yet achievable for a tiny model.
- Temperature tuned appropriately: lower for deterministic output (\`0.1\`), higher for jokes (\`0.8\`).
- Uses \`jq\` to extract clean response.

> ğŸ’¬ Tip: For ultra-low-latency use, try \`Q3_K_M\` or \`Q4_K_S\` on older laptops.

## Verification

Check integrity:

\`\`\`bash
sha256sum -c ../SHA256SUMS.txt
\`\`\`

## Usage

Compatible with:
- [LM Studio](https://lmstudio.ai) â€“ local AI model runner
- [OpenWebUI](https://openwebui.com) â€“ self-hosted AI interface
- [GPT4All](https://gpt4all.io) â€“ private, offline AI chatbot
- Directly via \`llama.cpp\`

## License

Apache 2.0 â€“ see base model for full terms.
EOF

    echo "âœ… Generated per-model card: $DIRNAME/README.md"
done

# -------------------------------
# STEP 5: Generate Shared MODELFILE
# -------------------------------

cat > MODELFILE << 'EOF'
# MODELFILE for Qwen3-0.6B-GGUF
# Used by LM Studio, OpenWebUI, GPT4All, etc.

context_length: 32768
embedding: false
f16: cpu

# Chat template using ChatML (used by Qwen)
prompt_template: >-
        <|im_start|>system
       You are a helpful assistant.<|im_end|>
        <|im_start|>user
       {prompt}<|im_end|>
        <|im_start|>assistant

# Stop sequences help end generation cleanly
stop: "<|im_end|>"
stop: "<|im_start|>"

# Default sampling
temperature: 0.6
top_p: 0.95
top_k: 20
min_p: 0.0
repeat_penalty: 1.1
EOF

sed -i "s|Qwen3-1.7B|$MODEL_NAME|" MODELFILE

echo "âœ… Shared MODELFILE generated!"

# -------------------------------
# STEP 6a: Ensure HF Repo Exists
# -------------------------------

echo "ğŸ” Ensuring Hugging Face repo exists: $HF_REPO"

if ! command -v huggingface-cli &> /dev/null; then
    echo "ğŸ“¦ Installing huggingface_hub..."
    pip install huggingface_hub --quiet
fi

python3 -c "
from huggingface_hub import create_repo
repo_id = '$HF_REPO'
try:
    create_repo(repo_id, repo_type='model', private=False, exist_ok=True)
    print(f'âœ… Repository {repo_id} is ready.')
except Exception as e:
    print(f'âŒ Failed to create repo: {e}')
    exit(1)
"

# -------------------------------
# STEP 6b: Upload to Hugging Face
# -------------------------------

echo "ğŸ“¤ Final files to upload:"
find . -type f -name "*.gguf" -o -name "README.md" -o -name "MODELFILE" -o -name "SHA256SUMS.txt" | xargs ls -lh

read -p "ğŸ‘‰ Continue with upload? [Y/n] " -r
echo

if [[ $REPLY =~ ^[Nn]$ ]]; then
    echo "â­ï¸ Skipped upload. Files ready in $OUTPUT_DIR"
    exit 0
fi

echo "ğŸš€ Uploading all files to $HF_REPO_URL"
hf upload \
  "$HF_REPO" \
  . \
  --repo-type=model \
  --commit-message "$COMMIT_MSG"

echo
echo "ğŸ‰ Success! Your model has been uploaded."
echo "ğŸŒ View it at: $HF_REPO_URL"